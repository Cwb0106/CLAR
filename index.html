<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>CLAR: Learning 3D Representations for Robotic Manipulation</title>
  
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <style>
    .method-img { margin-top: 20px; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); }
    .table-container { margin-top: 20px; }
    .results-img { width: 100%; border-radius: 8px; }
  </style>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CLAR: Learning 3D Representations for Robotic Manipulation by Fusing Masked Reconstruction with Multi-Level Contrastive Alignment</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Wenbo Cui<sup>* 1,2,3</sup>,</span>
              <span class="author-block">Chengyang Zhao<sup>* 4</sup>,</span>
              <span class="author-block">Yuhui Chen<sup>1,2</sup>,</span>
              <span class="author-block">Haoran Li<sup>1,2</sup>,</span>
              <span class="author-block">Zhizheng Zhang<sup>5</sup>,</span>
              <span class="author-block">Dongbin Zhao<sup>1,2</sup>,</span>
              <span class="author-block">He Wang<sup>† 5,6</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CASIA, <sup>2</sup>UCAS, <sup>3</sup>BAAI, <sup>4</sup>CMU, <sup>5</sup>Galbot, <sup>6</sup>Peking University</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution, <sup>†</sup>Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2507.08262.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2507.08262" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          
          <div class="has-text-centered">
            <img src="static/images/data_modalities_v2.jpg" alt="2D vs 3D Modalities" class="method-img">
            <p class="is-size-6 mt-2"><i>Comparison of 2D and 3D Modalities: CLAR leverages 3D point clouds to resolve multi-view ambiguity[cite: 52, 55].</i></p>
          </div>

          <div class="content has-text-justified mt-5">
            <p>
              The spatial information inherent in 3D point clouds is crucial for robotic manipulation[cite: 3]. However, existing 3D pre-training methods face a fundamental trade-off: Masked Autoencoding (MAE) excels at capturing spatial-geometric features but lacks semantics, whereas contrastive learning is ill-suited for fine-grained details[cite: 4, 24].
            </p>
            <p>
              To address these challenges, we propose <strong>CLAR</strong>, a novel 3D pre-training framework that synergizes global understanding with fine-grained local alignment[cite: 5]. CLAR unifies MAE with global cross-modal contrastive learning and introduces an adaptive alignment mechanism leveraging <strong>deformable attention</strong> to force precise 3D-to-2D correspondences[cite: 6, 7].
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Methodology</h2>
          <img src="static/images/pipeline.jpg" alt="CLAR Pipeline" class="method-img">
          <div class="content has-text-justified mt-4">
            <p>
              <strong>(a) The CLAR Pre-training Framework:</strong> We enhance spatial understanding via MAE and semantic comprehension through contrastive learning[cite: 110]. To capture fine-grained local details, we supplement the global contrastive loss with an adaptive local feature alignment mechanism using deformable attention[cite: 111, 147].
            </p>
            <p>
              <strong>(b) Resolving Contextual Mismatch:</strong> Traditional point cloud cropping removes background context, leading to feature discrepancy[cite: 112]. Our adaptive local alignment strategy ensures learning focuses on meaningful, shared information between modalities[cite: 59].
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Experimental Results</h2>
      
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h3 class="title is-4">Simulation Benchmarks (MetaWorld & RLBench)</h3>
          <img src="static/images/main.png" alt="Table I Results" class="results-img">
          <p class="is-size-6 mt-2">CLAR achieves state-of-the-art success rates (82.6% on MetaWorld and 82.0% on RLBench), outperforming 2D baselines and existing 3D pre-training methods[cite: 66, 219].</p>
        </div>
      </div>

      <div class="columns is-centered mt-6">
        <div class="column is-full-width">
          <h3 class="title is-4">Real-World Robot Experiments (Franka Emika)</h3>
          <div class="columns">
            <div class="column is-6">
              <img src="static/images/real_world_exp_01.jpg" alt="Real World Success Rates" class="results-img">
            </div>
            <div class="column is-6">
              <img src="static/images/real_world_01.jpg" alt="Real World Visualizations" class="results-img">
            </div>
          </div>
          <p class="is-size-6 mt-2">CLAR demonstrates robust generalization across multiple real-world tasks including <i>pick-banana</i> and <i>open-drawer</i>, achieving 83.0% mean success rate[cite: 66, 312].</p>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{cui2025clar,
  title={CLAR: Learning 3D Representations for Robotic Manipulation by Fusing Masked Reconstruction with Multi-Level Contrastive Alignment},
  author={Cui, Wenbo and Zhao, Chengyang and Chen, Yuhui and Li, Haoran and Zhang, Zhizheng and Zhao, Dongbin and Wang, He},
  journal={arXiv preprint arXiv:2507.08262},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>This website is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.</p>
      </div>
    </div>
  </footer>

</body>
</html>
