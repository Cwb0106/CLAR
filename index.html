<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CLAR: Learning 3D Representations for Robotic Manipulation</title>
  
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <style>
    .method-img { margin-top: 20px; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); }
    .results-img { width: 100%; border-radius: 8px; }
    /* 左侧蓝色修饰条 */
    .content-card { border-left: 5px solid #2563eb; padding-left: 20px; height: 100%; display: flex; flex-direction: column; justify-content: center; }
  </style>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CLAR: Learning 3D Representations for Robotic Manipulation by Fusing Masked Reconstruction with Multi-Level Contrastive Alignment</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Wenbo Cui<sup>* 1,2,3</sup>,</span>
              <span class="author-block">Chengyang Zhao<sup>* 4</sup>,</span>
              <span class="author-block">Yuhui Chen<sup>1,2</sup>,</span>
              <span class="author-block">Haoran Li<sup>1,2</sup>,</span>
              <span class="author-block">Zhizheng Zhang<sup>5</sup>,</span>
              <span class="author-block">Dongbin Zhao<sup>1,2</sup>,</span>
              <span class="author-block">He Wang<sup>† 5,6</sup></span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CASIA, <sup>2</sup>UCAS, <sup>3</sup>BAAI, <sup>4</sup>CMU, <sup>5</sup>Galbot, <sup>6</sup>Peking University</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution, <sup>†</sup>Corresponding Author</small></span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2507.08262.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2507.08262" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="has-text-centered">
            <img src="static/images/data_modalities_v2.png" alt="2D vs 3D Modalities" class="method-img">
            <p class="is-size-6 mt-2"><i>Comparison of 2D and 3D Modalities: CLAR leverages 3D point clouds to resolve multi-view ambiguity.</i></p>
          </div>
          <div class="content has-text-justified mt-5">
            <p>
              The spatial information inherent in 3D point clouds is crucial for robotic manipulation. However, existing 3D pre-training methods face a fundamental trade-off: Masked Autoencoding (MAE) excels at capturing spatial-geometric features but lacks semantics, whereas contrastive learning is ill-suited for the fine-grained details required for manipulation tasks.
            </p>
            <p>
              To address these challenges, we propose <strong>CLAR</strong>, a novel 3D pre-training framework that synergizes global understanding with fine-grained local alignment. CLAR unifies MAE with global cross-modal contrastive learning and introduces an adaptive alignment mechanism leveraging <strong>deformable attention</strong> to force precise 3D-to-2D correspondences.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Methodology</h2>
          <img src="static/images/pipeline.png" alt="CLAR Pipeline" class="method-img">
          <div class="content has-text-justified mt-4">
            <p>
              <strong>(a) The CLAR Pre-training Framework:</strong> We enhance spatial understanding via MAE and semantic comprehension through contrastive learning. To capture the fine-grained local details, we supplement the global contrastive loss with an adaptive local feature alignment mechanism using deformable attention.
            </p>
            <p>
              <strong>(b) Resolving Contextual Mismatch:</strong> Traditional point cloud cropping removes background context, leading to feature discrepancy. Our adaptive local alignment strategy ensures learning focuses on meaningful, shared information between modalities.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Experimental Results</h2>
      
      <div class="content mt-6">
        <h3 class="title is-4">Simulation Benchmarks</h3>
        <div class="columns is-vcentered">
          <div class="column is-7">
            <img src="static/images/main.png" alt="Simulation Results" class="results-img">
          </div>
          <div class="column is-5">
            <p>CLAR achieves state-of-the-art success rates (<strong>82.6%</strong> on MetaWorld and <strong>82.0%</strong> on RLBench), significantly outperforming 2D baselines and existing 3D pre-training methods.</p>
          </div>
        </div>
      </div>

      <hr>

      <div class="content mt-6">
        <h3 class="title is-4">Real-World Robot Experiments (Franka Emika)</h3>
        
        <div class="columns is-centered">
          <div class="column is-full">
            <img src="static/images/real_world_01.png" alt="Real World Visualizations" style="border-radius: 10px; width: 100%;">
            <p class="has-text-centered is-size-7 mt-2"><i>Qualitative results across five real-world tasks.</i></p>
          </div>
        </div>

        <div class="columns is-vcentered mt-5">
          <div class="column is-6">
            <img src="static/images/real_world_exp_01.png" alt="Success Rate Chart" class="results-img">
          </div>
          <div class="column is-6">
            <div class="content-card">
              <p class="is-size-5"><strong>83.0% Mean Success Rate</strong></p>
              <p class="is-size-6 mt-2">
                CLAR significantly outperforms baselines in challenging real-world tasks. Its unified 3D pre-training allows it to better comprehend object geometries and spatial relations, resolving the modality mismatch common in previous works.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{cui2025clar,
  title={CLAR: Learning 3D Representations for Robotic Manipulation by Fusing Masked Reconstruction with Multi-Level Contrastive Alignment},
  author={Cui, Wenbo and Zhao, Chengyang and Chen, Yuhui and Li, Haoran and Zhang, Zhizheng and Zhao, Dongbin and Wang, He},
  journal={arXiv preprint arXiv:2507.08262},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>This website is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.</p>
      </div>
    </div>
  </footer>

</body>
</html>
